{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc252d8b",
   "metadata": {},
   "source": [
    "# Kaggle competition: Natural Language Processing with Disaster Tweets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03686bd0",
   "metadata": {},
   "source": [
    "### Brief description of the problem and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932f82b",
   "metadata": {},
   "source": [
    "This project is to participate in the Kaggle learning competition to build an NLP model which does binary classification\n",
    "over a data set of about 10,000 English language tweets which may or may not be describing disasters. This is a supervised learning problem. \n",
    "\n",
    "The data consist of a training data file train.csv (7613 rows) and a test data file test.csv (3263 rows)\n",
    "\n",
    "The train.csv has the following columns:\n",
    "\n",
    "* id -- an integer id for each sample tweet\n",
    "\n",
    "* keyword -- a category for the tweet with 222 unique values, 61 missing\n",
    "\n",
    "* location -- location name with 3341 unique values, 2533 missing\n",
    "\n",
    "* text -- the text of the tweet\n",
    "\n",
    "* target -- the 0/1 label for binary classification where 1 = disaster\n",
    "\n",
    "The test.csv has the same first 4 columns but not the target value which we need to predict. The id is not significant for training, but is needed for the test data because we need to include it with the predictions submitted to Kaggle.\n",
    "\n",
    "In the training set, the  two classes are not quite evenly balanced, but are close with 42.9% in the positive (disaster) class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f919fc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8fe85d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d8470d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5427.152927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3146.427221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "count   3263.000000\n",
       "mean    5427.152927\n",
       "std     3146.427221\n",
       "min        0.000000\n",
       "25%     2683.000000\n",
       "50%     5500.000000\n",
       "75%     8176.000000\n",
       "max    10875.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b4ca2",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) Data Cleaning and Preparation\n",
    "\n",
    "The general plan is to build some type of binary classification recurrent neural network model on the sequence of words in the tweets. The assignment suggests using word embeddings. I chose to use GloVe pretrained embeddings from https://nlp.stanford.edu/projects/glove/ . In particular I picked the glove.6B.zip data set which has a 400K vocabulary extracted and trained from Wikipedia 2014 and Gigaword 5. This data set has a choice of 4 sets of word vectors of dimension 50, 100, 200, or 300.\n",
    "\n",
    "This dataset provides a dictionary to look up word vectors from lower cased word tokens. So my strategy for cleaning the data is to lower case all the letters in the tweets, while discarding all punctuation, numbers, and anything else not alpha characters, then split the resulting string on blanks, to get work tokens. I will then look up each token in the GloVe dictionary, and skip any tokens that don't match a known word embedding.\n",
    "\n",
    "In this process I found that some of the tweets had no useable words.  I could have skipped these samples for the training set, but they need to be preserved in the test set because we must provide a prediction for each of the test data in submitting to Kaggle. To treat the training and test data sets the same, I chose to substitute the single word \"neutral\" for any tweet that had no useable tokents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7a67f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed seconds = 9.88432240486145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Function to Load the GloVe dictionary of word embeddings.\n",
    "## I experimented with word vectors of 50, 100, and 200 dimension from GloVe, but 100 seems to work best for this problem.\n",
    "\n",
    "def get_glove_vectors(filename=\"data/glove.6B.100d.txt\"):\n",
    "    ## function from https://campus.datacamp.com/courses/recurrent-neural-networks-for-language-modeling-in-python/rnn-architecture?ex=7\n",
    "    # Get all word vectors from pre-trained model\n",
    "    glove_vector_dict = {}\n",
    "    with open(filename, encoding=\"UTF-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = values[1:]\n",
    "            glove_vector_dict[word] = np.asarray(coefs, dtype='float32')\n",
    "    return glove_vector_dict\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "glove_vector_dict = get_glove_vectors()\n",
    "\n",
    "end = time.time()\n",
    "print(f'elapsed seconds = {end - start}')\n",
    "type(glove_vector_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef239f65",
   "metadata": {},
   "source": [
    "I chose to ignore the Location column in the data and I added the keyword column as additional words at the end of each tweet.\n",
    "\n",
    "At this point I have the train and test datasets in pandas data frames df_train and df_test, exactly as read from the CSV files. Initially I tried just discarding the keyword column. But I changed this to the simple approach of appending the keywords as additional words at the end of each tweet. The following code does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec4e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Previous models ignored the keyword column in the training and test data.\n",
    "\n",
    "## To incorporate the keyword, we will try just appending the keywords (when present) as\n",
    "## an additional one or two tokens at the end of each tweet text.\n",
    "\n",
    "## This function modifies the pandas dataframe df by\n",
    "## appending the keyword (if present) to the end of the tweet.\n",
    "## Keywords like 'airplane%20accident' are split into two words.\n",
    "## It also writes the modified dataframe to a csv file (for debugging).\n",
    "def add_keyword(df, filename=None):\n",
    "    df.loc[df['keyword'].isna()==False,'text'] = df['text'] + ' ' + df['keyword'].str.replace('%20',' ')\n",
    "    if filename:\n",
    "        df.to_csv(f'data/{filename}',index=False)\n",
    "\n",
    "add_keyword(df_train, filename='df_train.csv')\n",
    "add_keyword(df_test, filename='df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6218eec",
   "metadata": {},
   "source": [
    "Here is the raw text from a few example tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb106e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do you like pasta?',\n",
       " 'The end!',\n",
       " '@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C ablaze',\n",
       " 'We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw ablaze',\n",
       " '#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi ablaze',\n",
       " 'Crying out for more! Set me ablaze ablaze']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train.loc[29:34,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56e3aedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"+ Nicole Fletcher one of a victim of crashed airplane few times ago. \\n\\nThe accident left a little bit trauma for her. Although she's \\n\\n+ airplane accident\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = df_train[df_train['id']==232]['text']\n",
    "s1 = list(s1)[0]\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ac33e",
   "metadata": {},
   "source": [
    "Here we define a function to clean up the tweets by removing all non alpha characters, lower case the text, and split the string into word tokens on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5747f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_up_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Clean up the content of one tweet, removing punctuation and numbers. \n",
    "    \n",
    "    Parameters:\n",
    "    tweet(str):The text of the tweet\n",
    "    \n",
    "    Returns:\n",
    "    word_list: A list of pure alphabetic words in lower case\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Remove all characters execept alphabetic chars and space,\n",
    "    ## convert to lower case and split on space.\n",
    "    word_list = re.sub('[^A-Za-z ]+','',tweet).lower().split(' ')\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3ee53c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'nicole',\n",
       " 'fletcher',\n",
       " 'one',\n",
       " 'of',\n",
       " 'a',\n",
       " 'victim',\n",
       " 'of',\n",
       " 'crashed',\n",
       " 'airplane',\n",
       " 'few',\n",
       " 'times',\n",
       " 'ago',\n",
       " 'the',\n",
       " 'accident',\n",
       " 'left',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'trauma',\n",
       " 'for',\n",
       " 'her',\n",
       " 'although',\n",
       " 'shes',\n",
       " '',\n",
       " 'airplane',\n",
       " 'accident']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Same as the last example tweet after cleaning\n",
    "clean_up_tweet(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19e872",
   "metadata": {},
   "source": [
    "In preparing the data for training, I want to reserve 20% for validation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db61922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 5)\n",
      "(1523, 5)\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(df_train, train_size=0.8, shuffle=True, random_state=42)\n",
    "print(train.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623c34b",
   "metadata": {},
   "source": [
    "Here we are applying the clean_up_tweet function to produce clean lower case word tokens for each of 3 data sets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95115c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090,)\n",
      "(1523,)\n",
      "(3263,)\n"
     ]
    }
   ],
   "source": [
    "train_x = train['text'].map(clean_up_tweet)\n",
    "valid_x = valid['text'].map(clean_up_tweet)\n",
    "test_x = df_test['text'].map(clean_up_tweet)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    "print(test_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a2753",
   "metadata": {},
   "source": [
    "Here we extract the target values for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07973f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 0.]\n",
      "[0. 0. 0. 1. 1.]\n",
      "0.43054187192118226\n",
      "0.4261326329612607\n"
     ]
    }
   ],
   "source": [
    "train_y = np.array(train['target'], dtype=np.float32)\n",
    "valid_y = np.array(valid['target'], dtype=np.float32)\n",
    "print(train_y[:5])\n",
    "print(train_y[-5:])\n",
    "print(np.sum(train_y)/len(train_y))\n",
    "print(np.sum(valid_y)/len(valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d701680",
   "metadata": {},
   "source": [
    "Above we confirmed that the train and validation sets have about the same class proportions for the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8e664",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Once we have a sequence of word vectors for each tweet, \n",
    "we will also need to pad each sequence to a uniform length before training with a recurrent NN model. \n",
    "The following code counts the words in each tweet and finds the max lengths. Based on these counts,\n",
    "I chose to pad all the sequences to length 56, which covers all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "270375e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "31\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "train['tweet_word_counts'] = [len(x) for x in train_x]\n",
    "valid['tweet_word_counts'] = [len(x) for x in valid_x]\n",
    "df_test['tweet_word_counts'] = [len(x) for x in test_x]\n",
    "print(np.max(train['tweet_word_counts']) )\n",
    "print(np.max(valid['tweet_word_counts']) )\n",
    "print(np.max(df_test['tweet_word_counts']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a98655",
   "metadata": {},
   "source": [
    "Now we are ready to replace the cleaned word tokens in the tweets with word embeddings from the GloVe data set. I am using the 100d data file that has vectors of 100 floats for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d849a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_word_embeddings(word_lists, pad_to=56):\n",
    "    ## We plan to replace all the words in the tweets\n",
    "    ## with embeddings from the GloVe dictionary, skipping\n",
    "    ## any words not found, and also padding the sequence \n",
    "    ## of embeddings to a fixed length.\n",
    "    \n",
    "    ## If none of the words match for a given tweet we will substitute\n",
    "    ## a with place holder vector of one word, \"neutral\".\n",
    "    d = glove_vector_dict\n",
    "    neutral = d[\"neutral\"]\n",
    "    placeHolder = np.array([neutral])\n",
    "    padNeutral = pad_sequences(placeHolder.T, pad_to, dtype='float32')\n",
    "    outer = []\n",
    "    for word_list in word_lists:\n",
    "        enc_list = []\n",
    "        for word in word_list:\n",
    "            if(type(d.get(word)) is np.ndarray):\n",
    "                enc_list.append(d.get(word))\n",
    "        if(len(enc_list) > 0):\n",
    "            enc_array = np.array(enc_list)\n",
    "            pad = pad_sequences(enc_array.T, pad_to, dtype='float32')\n",
    "            outer.append(pad.T)\n",
    "        else:\n",
    "            outer.append(padNeutral.T)\n",
    "    return np.array(outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61173fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed seconds = 2.3428685665130615\n",
      "(6090, 56, 100)\n",
      "(1523, 56, 100)\n",
      "(3263, 56, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "X_train = glove_word_embeddings(train_x)\n",
    "X_valid = glove_word_embeddings(valid_x)\n",
    "X_test = glove_word_embeddings(test_x)\n",
    "end = time.time()\n",
    "print(f'elapsed seconds = {end - start}')\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Architecture\n",
    "\n",
    "Model 1 -- Single Layer LSTM, 128 units per layer\n",
    "\n",
    "Model 2 -- Two Layer LSTM, 128 units per layer\n",
    "\n",
    "Model 3 -- Three Layer LSTM, 128 units per layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81695379",
   "metadata": {},
   "source": [
    "## Model 1 -- Single Layer LSTM, 128 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca7b5a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 128\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, input_shape=(None, 100), return_sequences=False, dropout=DROPOUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c690b58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "305/305 [==============================] - 7s 12ms/step - loss: 0.4826 - accuracy: 0.7787 - val_loss: 0.4208 - val_accuracy: 0.8214\n",
      "Epoch 2/100\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.4300 - accuracy: 0.8107 - val_loss: 0.4383 - val_accuracy: 0.7984\n",
      "Epoch 3/100\n",
      "305/305 [==============================] - 3s 11ms/step - loss: 0.4145 - accuracy: 0.8200 - val_loss: 0.4060 - val_accuracy: 0.8345\n",
      "Epoch 4/100\n",
      "305/305 [==============================] - 3s 11ms/step - loss: 0.3916 - accuracy: 0.8274 - val_loss: 0.4061 - val_accuracy: 0.8326\n",
      "Epoch 5/100\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.3832 - accuracy: 0.8343 - val_loss: 0.4104 - val_accuracy: 0.8326\n",
      "Epoch 6/100\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.3627 - accuracy: 0.8412 - val_loss: 0.4394 - val_accuracy: 0.8089\n",
      "Epoch 7/100\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.3458 - accuracy: 0.8489 - val_loss: 0.4311 - val_accuracy: 0.8201\n",
      "Epoch 8/100\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.3290 - accuracy: 0.8614 - val_loss: 0.4326 - val_accuracy: 0.8221\n",
      "Epoch 9/100\n",
      "302/305 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8733Restoring model weights from the end of the best epoch: 3.\n",
      "305/305 [==============================] - 3s 10ms/step - loss: 0.3022 - accuracy: 0.8724 - val_loss: 0.4672 - val_accuracy: 0.8070\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, train_y, \n",
    "                    batch_size=20, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_valid,valid_y),\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ece78588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 1s 5ms/step\n",
      "77/77 [==============================] - 0s 4ms/step\n",
      "acc 0.8240315167432699\n",
      "recall 0.7580893682588598\n",
      "precision 0.8159203980099502\n",
      "F1 0.7859424920127794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>763</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  763  111\n",
       "1  157  492"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num = 18\n",
    "\n",
    "predict_proba = model.predict(X_test,batch_size=20)\n",
    "predict = (predict_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(df_test['id'])\n",
    "submission['target']=predict\n",
    "\n",
    "submission.to_csv(f'data/submission{sub_num}.csv',index=False)\n",
    "submission\n",
    "\n",
    "yp_proba = model.predict(X_valid, batch_size=20)\n",
    "yp = (yp_proba > 0.402).astype(int)\n",
    "print('acc', accuracy_score(valid_y, yp))\n",
    "print('recall', recall_score(valid_y, yp))\n",
    "print('precision', precision_score(valid_y, yp))\n",
    "print('F1', f1_score(valid_y, yp))\n",
    "pd.DataFrame(confusion_matrix(valid_y,yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4290c31",
   "metadata": {},
   "source": [
    "## Model 2 -- Two Layer LSTM, 128 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "737c2777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, None, 128)         117248    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 248,961\n",
      "Trainable params: 248,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 128\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, input_shape=(None, 100), return_sequences=True, dropout=DROPOUT))\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, return_sequences=False, dropout=DROPOUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52cbfa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "305/305 [==============================] - 9s 21ms/step - loss: 0.4826 - accuracy: 0.7795 - val_loss: 0.4318 - val_accuracy: 0.7984\n",
      "Epoch 2/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.4321 - accuracy: 0.8107 - val_loss: 0.4331 - val_accuracy: 0.8102\n",
      "Epoch 3/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.4187 - accuracy: 0.8227 - val_loss: 0.4262 - val_accuracy: 0.8345\n",
      "Epoch 4/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.4037 - accuracy: 0.8213 - val_loss: 0.4144 - val_accuracy: 0.8332\n",
      "Epoch 5/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.3880 - accuracy: 0.8348 - val_loss: 0.4134 - val_accuracy: 0.8273\n",
      "Epoch 6/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.3680 - accuracy: 0.8386 - val_loss: 0.4213 - val_accuracy: 0.8260\n",
      "Epoch 7/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.3502 - accuracy: 0.8483 - val_loss: 0.4349 - val_accuracy: 0.8221\n",
      "Epoch 8/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.3319 - accuracy: 0.8555 - val_loss: 0.4499 - val_accuracy: 0.8194\n",
      "Epoch 9/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.3122 - accuracy: 0.8683 - val_loss: 0.4829 - val_accuracy: 0.8175\n",
      "Epoch 10/100\n",
      "305/305 [==============================] - 5s 18ms/step - loss: 0.2888 - accuracy: 0.8749 - val_loss: 0.4940 - val_accuracy: 0.8142\n",
      "Epoch 11/100\n",
      "303/305 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.8851Restoring model weights from the end of the best epoch: 5.\n",
      "305/305 [==============================] - 6s 18ms/step - loss: 0.2708 - accuracy: 0.8854 - val_loss: 0.5251 - val_accuracy: 0.8135\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, train_y, \n",
    "                    batch_size=20, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_valid,valid_y),\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45253ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 2s 9ms/step\n",
      "77/77 [==============================] - 1s 8ms/step\n",
      "acc 0.8345370978332239\n",
      "recall 0.7303543913713405\n",
      "precision 0.8602540834845736\n",
      "F1 0.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>797</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  797   77\n",
       "1  175  474"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num = 18\n",
    "\n",
    "predict_proba = model.predict(X_test,batch_size=20)\n",
    "predict = (predict_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(df_test['id'])\n",
    "submission['target']=predict\n",
    "\n",
    "submission.to_csv(f'data/submission{sub_num}.csv',index=False)\n",
    "submission\n",
    "\n",
    "yp_proba = model.predict(X_valid, batch_size=20)\n",
    "yp = (yp_proba > 0.402).astype(int)\n",
    "print('acc', accuracy_score(valid_y, yp))\n",
    "print('recall', recall_score(valid_y, yp))\n",
    "print('precision', precision_score(valid_y, yp))\n",
    "print('F1', f1_score(valid_y, yp))\n",
    "pd.DataFrame(confusion_matrix(valid_y,yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5622c",
   "metadata": {},
   "source": [
    "## Model 3 -- Three Layer LSTM, 128 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a87284e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, None, 128)         117248    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, None, 128)         131584    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 380,545\n",
      "Trainable params: 380,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 128\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, input_shape=(None, 100), return_sequences=True, dropout=DROPOUT))\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))\n",
    "model.add(LSTM(units=UNITS_PER_LAYER, return_sequences=False, dropout=DROPOUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0401e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "305/305 [==============================] - 12s 28ms/step - loss: 0.4880 - accuracy: 0.7750 - val_loss: 0.4610 - val_accuracy: 0.8083\n",
      "Epoch 2/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.4468 - accuracy: 0.8033 - val_loss: 0.4264 - val_accuracy: 0.8181\n",
      "Epoch 3/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.4273 - accuracy: 0.8131 - val_loss: 0.4223 - val_accuracy: 0.8188\n",
      "Epoch 4/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.4085 - accuracy: 0.8204 - val_loss: 0.4308 - val_accuracy: 0.8109\n",
      "Epoch 5/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3974 - accuracy: 0.8309 - val_loss: 0.4232 - val_accuracy: 0.8240\n",
      "Epoch 6/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3780 - accuracy: 0.8350 - val_loss: 0.4133 - val_accuracy: 0.8313\n",
      "Epoch 7/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3640 - accuracy: 0.8432 - val_loss: 0.4371 - val_accuracy: 0.8221\n",
      "Epoch 8/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3411 - accuracy: 0.8591 - val_loss: 0.4355 - val_accuracy: 0.8240\n",
      "Epoch 9/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3211 - accuracy: 0.8662 - val_loss: 0.4484 - val_accuracy: 0.8142\n",
      "Epoch 10/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.3048 - accuracy: 0.8736 - val_loss: 0.4841 - val_accuracy: 0.8188\n",
      "Epoch 11/100\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.2740 - accuracy: 0.8854 - val_loss: 0.5376 - val_accuracy: 0.8030\n",
      "Epoch 12/100\n",
      "304/305 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.8880Restoring model weights from the end of the best epoch: 6.\n",
      "305/305 [==============================] - 8s 26ms/step - loss: 0.2644 - accuracy: 0.8880 - val_loss: 0.5321 - val_accuracy: 0.7925\n",
      "Epoch 12: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, train_y, \n",
    "                    batch_size=20, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_valid,valid_y),\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd27b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 3s 13ms/step\n",
      "77/77 [==============================] - 1s 12ms/step\n",
      "acc 0.8325673013788575\n",
      "recall 0.7411402157164869\n",
      "precision 0.846830985915493\n",
      "F1 0.7904683648315529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>787</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  787   87\n",
       "1  168  481"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num = 18\n",
    "\n",
    "predict_proba = model.predict(X_test,batch_size=20)\n",
    "predict = (predict_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(df_test['id'])\n",
    "submission['target']=predict\n",
    "\n",
    "submission.to_csv(f'data/submission{sub_num}.csv',index=False)\n",
    "submission\n",
    "\n",
    "yp_proba = model.predict(X_valid, batch_size=20)\n",
    "yp = (yp_proba > 0.402).astype(int)\n",
    "print('acc', accuracy_score(valid_y, yp))\n",
    "print('recall', recall_score(valid_y, yp))\n",
    "print('precision', precision_score(valid_y, yp))\n",
    "print('F1', f1_score(valid_y, yp))\n",
    "pd.DataFrame(confusion_matrix(valid_y,yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a61549",
   "metadata": {},
   "source": [
    "## Model 4 - Bi-Directional LSTM with 2 layers, 64 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13f1f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None, 100)]       0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, None, 128)        84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,425\n",
      "Trainable params: 183,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 64\n",
    "\n",
    "## Try switching to a Bidirectional LSTM model, as in this example\n",
    "## https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(None, 100) )\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))(inputs)\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=False, dropout=DROPOUT))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1,  activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a17995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "305/305 [==============================] - 14s 31ms/step - loss: 0.4913 - accuracy: 0.7726 - val_loss: 0.4206 - val_accuracy: 0.8293\n",
      "Epoch 2/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.4317 - accuracy: 0.8090 - val_loss: 0.4073 - val_accuracy: 0.8267\n",
      "Epoch 3/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.4199 - accuracy: 0.8174 - val_loss: 0.4091 - val_accuracy: 0.8299\n",
      "Epoch 4/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.4091 - accuracy: 0.8241 - val_loss: 0.4074 - val_accuracy: 0.8326\n",
      "Epoch 5/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.4001 - accuracy: 0.8227 - val_loss: 0.4109 - val_accuracy: 0.8253\n",
      "Epoch 6/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.3900 - accuracy: 0.8330 - val_loss: 0.4175 - val_accuracy: 0.8201\n",
      "Epoch 7/100\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.3792 - accuracy: 0.8365 - val_loss: 0.4097 - val_accuracy: 0.8253\n",
      "Epoch 8/100\n",
      "303/305 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8389Restoring model weights from the end of the best epoch: 2.\n",
      "305/305 [==============================] - 8s 27ms/step - loss: 0.3717 - accuracy: 0.8386 - val_loss: 0.4330 - val_accuracy: 0.8122\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, train_y, \n",
    "                    batch_size=20, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_valid,valid_y),\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30734863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 3s 20ms/step\n",
      "77/77 [==============================] - 2s 20ms/step\n",
      "acc 0.8325673013788575\n",
      "recall 0.7303543913713405\n",
      "precision 0.855595667870036\n",
      "F1 0.7880299251870323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>794</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  794   80\n",
       "1  175  474"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num = 18\n",
    "\n",
    "predict_proba = model.predict(X_test,batch_size=20)\n",
    "predict = (predict_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(df_test['id'])\n",
    "submission['target']=predict\n",
    "\n",
    "submission.to_csv(f'data/submission{sub_num}.csv',index=False)\n",
    "submission\n",
    "\n",
    "yp_proba = model.predict(X_valid, batch_size=20)\n",
    "yp = (yp_proba > 0.5).astype(int)\n",
    "print('acc', accuracy_score(valid_y, yp))\n",
    "print('recall', recall_score(valid_y, yp))\n",
    "print('precision', precision_score(valid_y, yp))\n",
    "print('F1', f1_score(valid_y, yp))\n",
    "pd.DataFrame(confusion_matrix(valid_y,yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64a71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff12710",
   "metadata": {},
   "source": [
    "## Model 5  - Bi-Directional LSTM with 3 layers, 64 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72cf1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None, 100)]       0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, None, 128)        84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, None, 128)        98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 282,241\n",
      "Trainable params: 282,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DROPOUT = 0.2\n",
    "UNITS_PER_LAYER = 64\n",
    "\n",
    "## Try switching to a Bidirectional LSTM model, as in this example\n",
    "## https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(None, 100) )\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))(inputs)\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=True, dropout=DROPOUT))(x)\n",
    "x = layers.Bidirectional(LSTM(units=UNITS_PER_LAYER, return_sequences=False, dropout=DROPOUT))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1,  activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "file_name = 'weights_{epoch:03d}_{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "checkpoint_filepath = os.path.join('.', 'SAVE_MODELS', file_name)\n",
    "\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "55dd6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "305/305 [==============================] - 21s 47ms/step - loss: 0.4848 - accuracy: 0.7711 - val_loss: 0.4503 - val_accuracy: 0.8043\n",
      "Epoch 2/100\n",
      "305/305 [==============================] - 13s 41ms/step - loss: 0.4374 - accuracy: 0.8094 - val_loss: 0.4168 - val_accuracy: 0.8280\n",
      "Epoch 3/100\n",
      "305/305 [==============================] - 12s 41ms/step - loss: 0.4238 - accuracy: 0.8130 - val_loss: 0.4163 - val_accuracy: 0.8214\n",
      "Epoch 4/100\n",
      "305/305 [==============================] - 13s 41ms/step - loss: 0.4146 - accuracy: 0.8207 - val_loss: 0.4086 - val_accuracy: 0.8326\n",
      "Epoch 5/100\n",
      "305/305 [==============================] - 13s 41ms/step - loss: 0.4057 - accuracy: 0.8199 - val_loss: 0.4593 - val_accuracy: 0.8011\n",
      "Epoch 6/100\n",
      "305/305 [==============================] - 12s 41ms/step - loss: 0.3906 - accuracy: 0.8302 - val_loss: 0.4115 - val_accuracy: 0.8207\n",
      "Epoch 7/100\n",
      "305/305 [==============================] - 13s 41ms/step - loss: 0.3831 - accuracy: 0.8304 - val_loss: 0.4189 - val_accuracy: 0.8313\n",
      "Epoch 8/100\n",
      "305/305 [==============================] - 13s 42ms/step - loss: 0.3678 - accuracy: 0.8455 - val_loss: 0.4197 - val_accuracy: 0.8267\n",
      "Epoch 9/100\n",
      "305/305 [==============================] - 13s 42ms/step - loss: 0.3627 - accuracy: 0.8397 - val_loss: 0.4182 - val_accuracy: 0.8260\n",
      "Epoch 10/100\n",
      "304/305 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8508Restoring model weights from the end of the best epoch: 4.\n",
      "305/305 [==============================] - 13s 42ms/step - loss: 0.3465 - accuracy: 0.8506 - val_loss: 0.4435 - val_accuracy: 0.8181\n",
      "Epoch 10: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, train_y, \n",
    "                    batch_size=20, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_valid,valid_y),\n",
    "                    callbacks=[earlyStopping,modelCheckpoint]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e111e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 3s 21ms/step\n",
      "77/77 [==============================] - 2s 21ms/step\n",
      "acc 0.8325673013788575\n",
      "recall 0.7303543913713405\n",
      "precision 0.855595667870036\n",
      "F1 0.7880299251870323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>794</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  794   80\n",
       "1  175  474"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_num = 19\n",
    "\n",
    "predict_proba = model.predict(X_test,batch_size=20)\n",
    "predict = (predict_proba > 0.5).astype(int)\n",
    "\n",
    "submission = pd.DataFrame(df_test['id'])\n",
    "submission['target']=predict\n",
    "\n",
    "submission.to_csv(f'data/submission{sub_num}.csv',index=False)\n",
    "submission\n",
    "\n",
    "yp_proba = model.predict(X_valid, batch_size=20)\n",
    "yp = (yp_proba > 0.5).astype(int)\n",
    "print('acc', accuracy_score(valid_y, yp))\n",
    "print('recall', recall_score(valid_y, yp))\n",
    "print('precision', precision_score(valid_y, yp))\n",
    "print('F1', f1_score(valid_y, yp))\n",
    "pd.DataFrame(confusion_matrix(valid_y,yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff69bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da46416",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1) Kaggle Natural Language Processing with Disaster Tweets https://www.kaggle.com/competitions/nlp-getting-started/overview\n",
    "    \n",
    "2) GloVe: Global Vectors for Word Representation https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "3) DataCamp Course: Recurrent Neural Networks for Language Modeling in Pythone: https://campus.datacamp.com/courses/recurrent-neural-networks-for-language-modeling-in-python\n",
    "\n",
    "4) Keras.io example code for Bi-Directional LSTM model: https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367404a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
